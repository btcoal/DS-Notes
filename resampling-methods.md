# Resampling Methods

## What is the difference between the parametric and non-parametric bootstrap?

The **parametric bootstrap** assumes that the data follows a specific parametric distribution (e.g., normal, exponential). In this approach, you first estimate the parameters of the assumed distribution from the observed data. Then, you generate bootstrap samples by drawing new samples from this estimated distribution. This method relies heavily on the correctness of the assumed distribution.

The **non-parametric bootstrap**, on the other hand, makes no assumptions about the underlying distribution of the data. Instead, it generates bootstrap samples by resampling with replacement directly from the observed data. This approach is more flexible and can be applied to a wider range of statistics, as it does not depend on any specific distributional form.

## How many bootstrap replications, $B$, should we use?
* Intervals: 1,000 - 10,000
* Point estimates: 100 - 1,000

## Jackknife basics: Explain how the jackknife estimator is constructed. What is the purpose of leaving one observation out, and how are bias and variance estimated?

### Implement a jackknife estimator in Python. Use NumPy arrays and a function `func` that computes a statistic. Return both the jackknife estimate and its variance.

### Compare jackknife and bootstrap methods.

## Under what conditions is the jackknife a poor choice?

### Why might one use the jackknife if the bootstrap usually performs better?

### Explain the difference between the standard bootstrap and the “.632 ” and “.632+ ” bootstrap estimators.

## Describe the idea behind permutation (randomization) tests and how they differ from bootstrap procedures.

Permutation tests are non-parametric methods used to test hypotheses by calculating the distribution of a test statistic under the null hypothesis through random rearrangements (permutations) of the observed data. They differ from bootstrap procedures in that permutation tests focus on hypothesis testing by evaluating how extreme the observed statistic is compared to the distribution generated by permutations, while bootstrap methods are primarily used for estimating the sampling distribution of a statistic (e.g., bias, variance, confidence intervals) by resampling with replacement from the observed data.

## What is the “m‑out‑of‑n” bootstrap (sub‑sampling) and when is it used?

## Define and contrast “out‑of‑bag” error and k‑fold cross‑validation. When is OOB error particularly useful?

## Explain “bagging” and its connection to the bootstrap.

## Discuss the bias‑variance trade‑off between leave‑one‑out cross‑validation (LOOCV) and k‑fold cross‑validation.

## What is stratified k‑fold cross‑validation and why is it important for classification tasks?

## Using scikit‑learn, show how to compute 10‑fold cross‑validation scores for a regression model and retrieve both train and test scores. What parameter controls the number of folds?

## Demonstrate how to use out‑of‑bag (OOB) scoring with a RandomForestRegressor in scikit‑learn. When would you expect the OOB score to be more efficient than k‑fold cross‑validation?

## Write a function that performs stratified k‑fold cross‑validation for a binary classification dataset using scikit‑learn. Use StratifiedKFold and return the average Area Under the ROC Curve (AUC).

## Time‑series CV coding: Implement a sliding‑window cross‑validation function for forecasting. The function should take a pandas Series, a window size w , horizon h , and a model‑fitting function. It should iteratively fit on series[i : i + w] and predict the next h steps. Compute the mean absolute error across all windows.

## Given a dataset with class imbalance, compare the impact of using standard k‑fold CV vs stratified k‑fold CV on model evaluation. Suggest how you would assess the difference in Python.

## Explain why bootstrap estimates might underestimate variability for small sample sizes and suggest remedies.

Bootstrap estimates can underestimate variability in small sample sizes because the resampling process may not capture the full range of variability present in the population. With a small dataset, the bootstrap samples are likely to be very similar to each other, leading to an artificially low estimate of variance. Additionally, extreme values or outliers may not be adequately represented in the bootstrap samples, further skewing the variability estimate.

Remedies for this issue include:
* Increasing the number of bootstrap replications (B) to ensure a more stable estimate.
* Using bias-corrected and accelerated (BCa) bootstrap intervals, which adjust for bias and skewness in the bootstrap distribution.
* Employing alternative resampling methods, such as the jackknife, which may provide better variance estimates for certain statistics.

## Discuss how you would use bootstrap or jackknife to estimate the standard error of a complex estimator—e.g., the parameter of a logistic regression—when the analytical formula is difficult or unavailable. Provide pros and cons of each method.

## In a regression setting, describe how cross‑validation can be used both to estimate test error and to select model complexity (e.g., polynomial degree or regularization parameter). Why is the validation‑set approach often insufficient?

## Many machine‑learning packages offer built‑in cross‑validation for hyper‑parameter tuning (e.g., GridSearchCV in scikit‑learn). What are potential pitfalls of using nested cross‑validation versus a single cross‑validation? Under what circumstances is nested cross‑validation essential?

## For a time‑series forecasting problem, how would you decide between using a rolling‑origin evaluation and a fixed‑origin (growing window) evaluation? Discuss trade‑offs in bias and variance[13].

## Describe how you could use permutation tests to evaluate feature importance in a random forest. What assumptions do permutation tests rely on and how do you control randomness?

## Explain the concept of “statistical efficiency” in the context of resampling methods. Compare the efficiency of jackknife, bootstrap and cross‑validation in terms of bias and variance for estimating a model’s prediction error.

## What challenges arise when applying resampling methods to dependent observations (e.g., spatial or temporal autocorrelation)? Suggest modifications to bootstrap and cross‑validation for these situations.

## In the context of bagging and random forests, why do we need to sample features (random feature subspace) in addition to sampling observations? How does this relate to bias, variance and correlation among trees?

## How does the bootstrap apply to Bayesian inference? Discuss the Bayesian bootstrap and its differences from the classical bootstrap.

## You are asked to deploy a machine‑learning model into production. Discuss how you would use resampling methods for model selection and risk estimation, and how you would communicate uncertainty to stakeholders.

## Describe the connection between the bootstrap and the plug‑in principle. Why does the bootstrap work well for estimators that are smooth functionals of the empirical distribution?

## Explain how cross‑validated residuals can be used for diagnostic plots (e.g., plotting CV residuals vs predicted values) and how they differ from training residuals.

## Discuss the computational cost of LOOCV for large datasets. When dealing with linear regression, how can LOOCV be computed efficiently using the hat matrix (leverages) rather than refitting n times[20]?

## What are influence functions and how are they related to jackknife and bootstrap methods? Discuss their use in robust statistics.

## Describe a scenario where cross‑validation yields an optimistically biased estimate of test error. How could you design a resampling strategy to mitigate this bias?

## Could you sketch out the 'family tree' of resampling methods we commonly use in data science? Where do bootstrap, jackknife, and cross-validation fit? What are their primary goals and how do they differ at a high level?

A comprehensive answer will draw a primary distinction between two major branches of the family tree: methods for **statistical inference** and methods for **model validation**.

* **Statistical Inference:** This branch focuses on estimating the properties of a sample statistic, such as its bias, variance, or confidence interval. The **bootstrap** and the **jackknife** are the principal members of this branch.<sup>3</sup> Their goal is to understand the uncertainty of an estimate derived from a sample, without necessarily having a predictive model. For example, they can be used to find the standard error of a sample median or a regression coefficient.
* **Model Validation:** This branch is concerned with estimating the predictive performance of a machine learning model on unseen data.<sup>6</sup> **Cross-validation** is the canonical example here. Its purpose is not to understand a statistic, but to estimate a property of a model, such as its out-of-sample accuracy or mean squared error.<sup>2</sup>

The **bootstrap** occupies a unique position, bridging both worlds. While its primary role is in statistical inference, it is also the core mechanism behind the machine learning ensemble technique of **bagging** (bootstrap aggregating), which is used to improve model performance.<sup>8</sup> A Staff-level candidate should not only make this distinction but articulate the different questions each branch answers. Inference asks, "How stable is my number?" Validation asks, "How well does my function predict?"

### Compare and contrast the jackknife and the bootstrap. When would you strongly prefer one over the other?"

The comparison should revolve around several key axes:

* **Resampling Mechanism:** The jackknife employs a deterministic, **leave-one-out** resampling strategy. For a dataset of size $n$, it generates exactly $n$ subsamples, each of size $n-1$.<sup>4</sup> In contrast, the bootstrap uses a stochastic, **sampling-with-replacement** strategy. It generates a large number of "bootstrap samples," each the same size as the original dataset, by drawing observations with replacement.<sup>11</sup>

* **Computational Cost:** The jackknife's deterministic nature means it requires exactly $n$ recalculations of the statistic. This makes it computationally faster for small datasets.<sup>13</sup> The bootstrap requires thousands of resamples (e.g., 1,000 to 10,000) to achieve a stable estimate of the sampling distribution, making it more computationally intensive.<sup>15</sup>

* **Applicability to Statistics:** This is the most critical distinction. The bootstrap is a more flexible and broadly applicable method. The jackknife is known to fail for non-smooth statistics, such as the median or other quantiles.<sup>13</sup> This failure is a direct consequence of its leave-one-out mechanism. Removing a single data point is often not disruptive enough to change the median, leading to a poor estimation of its variability. Bootstrap's sampling-with-replacement creates much more varied samples, providing a better empirical approximation of the sampling distribution for a wider class of problems.<sup>16</sup> The jackknife is, in fact, a linear approximation of the bootstrap, which helps explain its limitations with non-linear or non-smooth statistics.<sup>4</sup>
* **Reproducibility:** The jackknife is deterministic; given the same dataset, it will produce the exact same result every time. The bootstrap is stochastic; its results have Monte Carlo variability, meaning they will differ slightly on each run unless a random seed is fixed.<sup>15</sup>

A strong preference for the bootstrap arises when dealing with non-smooth statistics or when a full distributional estimate is needed (e.g., for constructing asymmetric confidence intervals). A candidate might prefer the jackknife for a quick, deterministic estimate of bias or variance for a smooth statistic on a small dataset where computational resources are a concern. For instance, a Staff-level candidate might state: "For estimating the bias of a regression coefficient from a small sample, I might start with the jackknife for a quick result. However, for constructing a reliable 95% confidence interval for the 90th percentile of user latency, I would exclusively use the bootstrap, as the jackknife is invalid for quantiles."

| Method | Resampling Strategy | Determinism | Primary Use Case | Computational Cost | Key Strengths | Key Limitations |
|--------|---------------------|-------------|------------------|--------------------|---------------|------------------|
| Bootstrap | Sampling with replacement | Stochastic | Statistical inference (CIs, SEs), model ensembling (bagging) | High (thousands of iterations) | Highly flexible, works for smooth and non-smooth statistics, provides full distribution | Can be slow, relies on sample representativeness
| Jackknife | Leave-one-out | Deterministic | Bias and variance estimation of a statistic | Low (n iterations) | Fast for small datasets, deterministic, good for bias correction of smooth statistics | Fails for non-smooth statistics (e.g., median), less flexible than bootstrap |
| k-Fold CV | Partitioning into k folds | Deterministic (given a random state) | Model performance evaluation (out-of-sample error) | Moderate (k iterations) | Efficient use of data, balances bias and variance | Assumes i.i.d. data, not for parameter inference |

### "Explain the core assumption of the bootstrap. What is the 'bootstrap world' analogy, and under what conditions might this assumption break down, leading to unreliable results?"

This question probes the conceptual foundation of the bootstrap. A candidate who cannot articulate its core assumption cannot be trusted to apply it correctly. The central idea of the bootstrap is that inference about a population from a sample can be modeled by resampling the sample itself.<sup>12</sup> This is often explained through the "bootstrap world" analogy: we treat the original sample as a perfect, miniature replica of the true population.<sup>17</sup> The logic follows that the process of drawing a resample from our original sample mimics the process of drawing our original sample from the true population.

The validity of this entire procedure hinges on one critical assumption: **the original sample must be representative of the underlying population**.<sup>19</sup> The limitations of the bootstrap are not arbitrary rules but are direct, logical consequences of this assumption breaking down.

Conditions where the assumption fails and results become unreliable include:

* **Small or Unrepresentative Samples:** If the original sample is too small or was collected in a biased way, it will not be a faithful representation of the population. The bootstrap will then simply amplify the biases and idiosyncrasies of that poor sample, leading to misleading inferences.<sup>18</sup>
* **Statistics Dependent on Extreme Values:** The bootstrap generates resamples by drawing from the original data. Therefore, it is impossible for a bootstrap sample to contain values outside the range of the original sample. This means the bootstrap will perform poorly when estimating statistics that depend heavily on the tails of the distribution, such as the maximum, minimum, or extreme quantiles (e.g., the 99.9th percentile).<sup>18</sup> The original sample is unlikely to have captured the true extremes of the population, and the bootstrap has no way to correct for this.
* **Dependent Data Structures:** The standard (naïve) bootstrap assumes that the data points are independent and identically distributed (i.i.d.). When this assumption is violated, as in time-series data (where observations are autocorrelated) or clustered data (where observations within a cluster are correlated), the bootstrap procedure will underestimate the true variability. A Staff-level candidate should not only identify this limitation but also proactively suggest solutions, such as the **block bootstrap** for time-series data, which resamples blocks of consecutive observations to preserve the dependency structure.<sup>18</sup>


### Example: User Sessions

    You are given a dataset of user session durations. The distribution is highly skewed, so the median is a more appropriate measure of central tendency than the mean. Write a Python function from scratch, using only NumPy, that takes this data and a confidence level, and returns a bootstrap confidence interval for the median.

1. Generate a large number of "bootstrap samples" by drawing from the original data with replacement. Each bootstrap sample must be the same size as the original data.
2. For each bootstrap sample, calculate the statistic of interest—in this case, the median. This creates a distribution of "bootstrap replicates."
3. Calculate the confidence interval from this distribution of replicates using the percentile method. For a 95% confidence interval, this means finding the 2.5th and 97.5th percentiles of the bootstrap replicate distribution.<sup>23</sup>

```python
import numpy as np

def bootstrap_ci_median(data, n_bootstraps=10000, ci=95):
    """
    Computes a bootstrap confidence interval for the median of a 1D data array.

    Args:
        data (np.array): A 1D numpy array of numerical data.
        n_bootstraps (int): The number of bootstrap samples to generate.
        ci (int): The desired confidence level (e.g., 95 for a 95% CI).

    Returns:
        tuple: A tuple containing the lower and upper bounds of the confidence interval.
    """
    # Initialize an array to store the bootstrap replicates of the median
    bootstrap_replicates = np.empty(n_bootstraps)

    # Get the size of the original data
    n_samples = len(data)

    # Generate bootstrap replicates
    for i in range(n_bootstraps):
        # Create a bootstrap sample by sampling with replacement
        bootstrap_sample = np.random.choice(data, size=n_samples, replace=True)

        # Calculate the median of the bootstrap sample and store it
        bootstrap_replicates[i] = np.median(bootstrap_sample)

    # Calculate the lower and upper percentile bounds
    lower_percentile = (100 - ci) / 2
    upper_percentile = 100 - lower_percentile

    # Calculate the confidence interval
    lower_bound, upper_bound = np.percentile(bootstrap_replicates, [lower_percentile, upper_percentile])
    return lower_bound, upper_bound

# Example usage:
skewed_data = np.random.exponential(scale=100, size=500)
lower, upper = bootstrap_ci_median(skewed_data, n_bootstraps=10000, ci=95)
print(f"95% Confidence Interval for the median: ({lower:.2f}, {upper:.2f})")
```

Extensions
* discuss the choice of n_bootstraps, explaining the trade-off between computational cost and the stability of the confidence interval estimate.
* advanced methods like the Bias-Corrected and accelerated (BCa) bootstrap interval, noting that it can provide more accurate coverage than the simple percentile method, especially for skewed distributions or small sample sizes.


### Bootstrapping and Random Forest

    Explain the role of bootstrapping in the Random Forest algorithm. How does it interact with feature selection to make Random Forests effective? And how can we leverage this process to get a 'free' measure of model performance without a separate validation set?

* **Role in Bagging:** Random Forest is an ensemble method built upon the concept of **Bagging**, which stands for **Bootstrap Aggregating**.<sup>9</sup> The process involves creating many bootstrap samples from the original training data. A separate, high-variance base learner—typically a deep, unpruned decision tree—is trained independently on each of these bootstrap samples.<sup>28</sup> The final prediction is made by aggregating the predictions of all the individual trees (e.g., by majority vote for classification or averaging for regression).
* **Variance Reduction:** The primary goal of bagging is to reduce the variance of the overall model. A single deep decision tree is a high-variance estimator; it is highly sensitive to the specific data it was trained on and is prone to overfitting. By training many trees on slightly different versions of the data (the bootstrap samples) and averaging their outputs, the noise and instability of the individual trees are smoothed out, resulting in a final model with much lower variance and better generalization performance.<sup>9</sup>
* **Interaction with Feature Selection:** The key innovation of Random Forest over simple bagging is the introduction of random feature selection at each split in the decision tree. While bootstrapping creates diversity in the *rows* of data each tree sees, this feature randomness creates diversity in the *columns*. This combination is crucial for **decorrelating the trees**. If a few features are very strong predictors, standard bagging would result in highly correlated trees, as most trees would select those same strong features for their top splits. By forcing each split to consider only a random subset of features, Random Forest ensures that the trees are more varied, which makes the averaging process more effective at reducing variance.<sup>29</sup> This decorrelation is the primary reason for Random Forest's superior performance over bagging decision trees.
* **Out-of-Bag (OOB) Error:** A powerful side effect of bootstrap sampling is the creation of an "out-of-bag" dataset for each tree. Since bootstrapping samples with replacement, on average, each bootstrap sample contains about 63.2% of the original data points, meaning about 36.8% () are left out.<sup>27</sup> These OOB points can be used as a natural validation set for the tree that was trained without them. To calculate the OOB error for the entire forest, each data point's prediction is made using only the trees for which that point was OOB. The aggregate error across all points provides a robust, unbiased estimate of the model's generalization error, often eliminating the need for a separate cross-validation or hold-out set.<sup>28</sup>

### "The jackknife is often described as a precursor to the bootstrap. Its primary use is for bias and variance estimation. Can you explain, from first principles, how leaving one observation out at a time allows us to estimate the bias of a statistic?"

This question probes a deep understanding of the jackknife mechanism, moving beyond simple recitation of formulas. The explanation should center on the concept of "pseudo-values" and the logic of extrapolation.<sup>1</sup>

The core logic is as follows:

1. We start with an estimate of our parameter,$\hat{\theta}$, calculated from the full sample of size $n$.
2. We then systematically create $n$ "leave-one-out" estimates, $\hat{\theta}_{(i)}$, by calculating the same statistic on the dataset with the $i$-th observation removed.
3. The collection of these $\hat{\theta}_{(i)}$ values tells us how sensitive our estimate is to the presence of each individual data point. The average of these leave-one-out estimates is $\bar{\hat{\theta}}_{LOO}$.
4. The difference between the full-sample estimate $\hat{\theta}$ and the average leave-one-out estimate $\bar{\hat{\theta}}_{LOO}$ is proportional to the bias. The formal jackknife estimate of bias is given by the formula: $\hat{Bias}_{jack} = (n-1)(\bar{\hat{\theta}}_{LOO} - \hat{\theta})$.<sup>4</sup>

The intuition behind this formula is that it measures the average change in the estimate when a single point is removed and then scales that change up by a factor of $n-1$ to approximate the total bias present in the $n$-point estimate. It effectively extrapolates from the small change observed by removing one point to estimate the systemic error across the whole sample. This procedure is most effective when the bias of the estimator can be expressed as a power series in $1/n$. The jackknife method is particularly good at estimating and removing the leading term of this bias (the $O(1/n)$ term), which is why it works well for "smooth" statistics where such an approximation holds.<sup>4</sup>


### "What is the relationship between jackknife resampling and Leave-One-Out Cross-Validation (LOOCV)? Are they the same thing? If not, how do they differ?"

This is a high-level conceptual question designed to identify true experts, as the procedural similarity between the two methods is a common source of confusion.

While both jackknife and LOOCV involve an identical procedure of iteratively creating  subsamples by leaving out one observation at a time, **they are not the same thing**. The fundamental difference lies in their **purpose** and in **what is calculated** in each iteration.<sup>4</sup>


* **Leave-One-Out Cross-Validation (LOOCV)** is a technique for **model evaluation**. In each of the  iterations, a model is trained on the  "kept" samples. The purpose of this model is to make a prediction for the single **left-out** sample. The performance metric (e.g., squared error, accuracy) is calculated by comparing this prediction to the actual value of the left-out sample. The final output of LOOCV is the average of these  performance metrics, which serves as an estimate of the model's generalization error.<sup>2</sup>
* **Jackknife Resampling** is a technique for **statistical inference** on an estimator. In each of the  iterations, the statistic of interest (e.g., the mean, variance, or a regression coefficient) is calculated using only the  **kept** samples. The left-out sample is ignored for this calculation. The final output of the jackknife is an aggregation (e.g., an average or variance) of these  statistics, which is then used to estimate a property (like the bias or standard error) of the original statistic calculated on the full sample.<sup>2</sup>

In short, LOOCV computes a metric on the **left-out** data point to assess a model, while the jackknife computes a statistic on the **kept** data points to assess an estimator. The confusion arises because in the specific context of using the jackknife procedure to generate a prediction for each point, the process becomes equivalent to LOOCV.<sup>30</sup> However, their primary goals remain distinct.

### "Explain the bias-variance trade-off in the context of k-fold cross-validation. How does the choice of 'k' impact the bias and variance of our model performance estimate?"

This is a fundamental concept in model evaluation. It is crucial to understand that this trade-off applies not to the model's intrinsic bias and variance, but to the **bias and variance of the performance estimate** obtained from cross-validation.

* **Bias of the Performance Estimate:** Bias refers to how much the average performance estimate from cross-validation deviates from the true generalization error of a model trained on the full dataset. As the number of folds, , increases, the size of the training set in each fold, , also increases. Models trained on more data tend to be more accurate. Therefore, as  increases, the models trained within the cross-validation loop are more similar to the final model trained on all data, and the performance estimate becomes **less biased** (i.e., less pessimistic). Leave-One-Out CV (where ) produces a nearly unbiased estimate of the true generalization error because each training set is of size , which is almost the full dataset size.<sup>31</sup>
* **Variance of the Performance Estimate:** Variance refers to how much the performance estimate would change if the entire cross-validation process were repeated on a different initial sample of data. As  increases, the training sets used in each fold become highly overlapping. For example, in LOOCV, any two training sets share  of their  data points. This high degree of overlap means the models trained in each fold are highly correlated with each other. Averaging highly correlated outputs does not effectively reduce variance. Therefore, as  increases, the variance of the performance estimate **increases**. LOOCV has the highest variance because the models are maximally correlated.<sup>31</sup> A low value of  (e.g.,  or ) results in less overlap between training sets, leading to less correlated models and a lower-variance, more stable performance estimate.

The common practice of choosing  or  is an empirically derived sweet spot that provides a good balance between the bias and variance of the performance estimate.<sup>32</sup>


### "Imagine you need to build a pipeline to predict customer churn. Your process involves imputing missing values, scaling numerical features, and then training a logistic regression model. You want to use 5-fold cross-validation to evaluate the model and GridSearchCV to tune the regularization parameter 'C'. Write the Python code using scikit-learn that correctly implements this entire pipeline, ensuring there is no data leakage."

This is a critical practical question. The single most common and dangerous mistake in using cross-validation is **data leakage**, which occurs when information from the validation set leaks into the training process, leading to overly optimistic performance estimates.<sup>34</sup> This typically happens when preprocessing steps like scaling or imputation are performed on the entire dataset *before* splitting it for cross-validation.

The correct approach is to use scikit-learn's Pipeline object. This encapsulates all preprocessing steps and the final model into a single object. When this pipeline is passed to a cross-validation tool like GridSearchCV, it ensures that for each fold, the preprocessing steps (e.g., SimpleImputer, StandardScaler) are fit *only* on the training portion of that fold. These fitted preprocessors are then used to transform both the training and validation portions, preventing any information from the validation set from influencing the training process.<sup>35</sup>

Furthermore, for a churn prediction problem, the target variable is often imbalanced. A Staff-level candidate should recognize this and proactively use StratifiedKFold to ensure that the proportion of churners and non-churners is preserved in each fold, leading to a more reliable evaluation.<sup>7</sup>

```python
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, StratifiedKFold

# Assume X_train, y_train are pre-existing training data
# X_train: pandas DataFrame or numpy array of features
# y_train: pandas Series or numpy array of binary churn labels (0 or 1)

# 1. Define the pipeline
# This encapsulates all steps: imputation, scaling, and the model.
pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])

# 2. Define the parameter grid for the logistic regression's 'C' parameter
# The syntax 'step_name__parameter_name' is used to target parameters within the pipeline.
param_grid = {
    'model__C': [0.01, 0.1, 1, 10, 100]
}

# 3. Define the cross-validation strategy
# StratifiedKFold is crucial for classification with imbalanced classes like churn.
cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 4. Set up and run GridSearchCV
# GridSearchCV will handle the cross-validation internally, applying the pipeline
# correctly to each fold to prevent data leakage.
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=cv_strategy,
    scoring='roc_auc',  # A good metric for imbalanced classification
    n_jobs=-1
)

# Fit the grid search to the training data
# grid_search.fit(X_train, y_train)

# 5. Display the results
print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best cross-validated ROC AUC score: {grid_search.best_score_:.4f}")

# The best estimator is now trained on the full X_train and ready for evaluation on a hold-out test set
best_model = grid_search.best_estimator_
```


### "Standard k-fold cross-validation assumes the data is i.i.d. Describe at least two scenarios where this assumption is violated and the appropriate cross-validation strategy you would use for each."

Real-world data is often messy and rarely i.i.d. (independent and identically distributed). This question separates candidates with textbook knowledge from those with practical experience handling complex data structures.

1. **Scenario: Time Series Data**
    * **Characteristic:** Observations have a temporal dependency; the order matters. The value at time  is often correlated with the value at time .<sup>38</sup>
    * **Problem with Standard K-Fold:** Random shuffling destroys the temporal order. This leads to a critical form of data leakage where the model is trained on data from the future to predict data in the past, an impossible real-world scenario. This results in a wildly overoptimistic performance estimate.<sup>39</sup>
    * **Recommended Strategy:** **Time Series Cross-Validation** (also known as rolling-origin or forward-chaining validation). This method creates folds that preserve the chronological order. In each split, the training set consists of earlier observations, and the validation set consists of later observations. The scikit-learn implementation is TimeSeriesSplit.<sup>38</sup>
2. **Scenario: Grouped or Clustered Data**
    * **Characteristic:** Observations are not independent but are grouped. Examples include multiple medical records from the same patient, multiple product reviews from the same user, or multiple rental listings from the same host. Observations within a group are likely to be more similar to each other than to observations from other groups.<sup>40</sup>
    * **Problem with Standard K-Fold:** Random shuffling will likely split data from the same group across both the training and validation sets. The model can then effectively "memorize" characteristics of a specific group (e.g., a particular patient's baseline health) and appear to perform well on the validation set, when in reality it has not learned a generalizable pattern.<sup>41</sup>
    * **Recommended Strategy:** **Group-Aware Cross-Validation**. This method ensures that all observations belonging to a single group are contained entirely within a single fold. They will appear together in either the training set or the validation set for any given split, but never in both. The scikit-learn implementations include GroupKFold, LeaveOneGroupOut, and StratifiedGroupKFold.<sup>41</sup>

<table>
  <tr>
   <td>
Scenario
   </td>
   <td>Data Characteristic
   </td>
   <td>Problem with Standard K-Fold
   </td>
   <td>Recommended CV Strategy
   </td>
   <td>scikit-learn Class
   </td>
  </tr>
  <tr>
   <td><strong>Standard</strong>
   </td>
   <td>Independent and Identically Distributed (IID)
   </td>
   <td>None
   </td>
   <td>K-Fold Cross-Validation
   </td>
   <td>KFold
   </td>
  </tr>
  <tr>
   <td><strong>Imbalanced Classification</strong>
   </td>
   <td>Uneven class distribution
   </td>
   <td>Folds may have unrepresentative class proportions, leading to biased evaluation
   </td>
   <td>Stratified K-Fold
   </td>
   <td>StratifiedKFold
   </td>
  </tr>
  <tr>
   <td><strong>Grouped/Clustered Data</strong>
   </td>
   <td>Observations within groups are correlated
   </td>
   <td>Data from the same group can appear in both train and validation sets, causing leakage
   </td>
   <td>Group K-Fold
   </td>
   <td>GroupKFold, LeaveOneGroupOut
   </td>
  </tr>
  <tr>
   <td><strong>Time-Dependent Data</strong>
   </td>
   <td>Observations are ordered chronologically and autocorrelated
   </td>
   <td>Shuffling destroys temporal order, leading to training on future data to predict the past
   </td>
   <td>Time Series Split (Forward Chaining)
   </td>
   <td>TimeSeriesSplit
   </td>
  </tr>
</table>

## Case Studies

### Google Poisson Bootstrap

* [Amir Najmi, "An introduction to the Poisson bootstrap." The Unofficial Google Data Science Blog](https://www.unofficialgoogledatascience.com/2015/08/an-introduction-to-poisson-bootstrap26.html)

* [Chamandy et al. 2012. Estimating Uncertainty for Massive Data Streams](./chamandy%20et%20al%20-%20estimating%20uncertainty%20for%20massive%20data%20streams%20(2012).pdf)

* [Hanley and MacGibbonb. 2006. Creating non-parametric bootstrap samples using Poisson frequencies](./bootstrap-hanley-macgibbon2006.pdf)

## References

* https://jbhender.github.io/Stats506/F20/ResamplingMethods.html

* https://www.stats.ox.ac.uk/pub/bdr/IAUL/Course1Notes6.pdf

* https://www.datasciencecentral.com/resampling-methods-comparison/

* https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/

* https://en.wikipedia.org/wiki/Out-of-bag_error

* https://bradleyboehmke.github.io/HOML/bagging.html

* https://www2.stat.duke.edu/~rcs46/lectures_2017/05-resample/05-cv.pdf

* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html

* https://dereksonderegger.github.io/578/5-resampling-methods.html

* https://online.stat.psu.edu/stat508/Lesson03.html

* https://otexts.com/fpp3/tscv.html

* https://nixtlaverse.nixtla.io/mlforecast/docs/how-to-guides/cross_validation.html

* https://people.duke.edu/~ccc14/sta-663/ResamplingAndMonteCarloSimulations.html

* https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html

* https://hastie.su.domains/MOOC-Slides/cv_boot.pdf

1. Resampling Data: Using a Statistical Jackknife - Washington University in St. Louis, accessed October 7, 2025, [https://www.math.wustl.edu/~sawyer/handouts/Jackknife.pdf](https://www.math.wustl.edu/~sawyer/handouts/Jackknife.pdf)
2. Cross-validation (statistics) - Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Cross-validation_(statistics)](https://en.wikipedia.org/wiki/Cross-validation_(statistics))
3. What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum - PMC - PubMed Central, accessed October 7, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC4784504/](https://pmc.ncbi.nlm.nih.gov/articles/PMC4784504/)
4. Jackknife resampling - Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Jackknife_resampling](https://en.wikipedia.org/wiki/Jackknife_resampling)
5. (PDF) Jackknife Resampling - ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/2829267_Jackknife_Resampling](https://www.researchgate.net/publication/2829267_Jackknife_Resampling)
6. Cross-Validation in Machine Learning: How to Do It Right - Neptune.ai, accessed October 7, 2025, [https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right)
7. Cross-Validation Techniques: k-Fold vs Stratified vs LOOCV - bugfree.ai, accessed October 7, 2025, [https://bugfree.ai/knowledge-hub/cross-validation-techniques-k-fold-stratified-loocv](https://bugfree.ai/knowledge-hub/cross-validation-techniques-k-fold-stratified-loocv)
8. Bootstrap Method - GeeksforGeeks, accessed October 7, 2025, [https://www.geeksforgeeks.org/maths/bootstrap-method/](https://www.geeksforgeeks.org/maths/bootstrap-method/)
9. What Is Bagging? | IBM, accessed October 7, 2025, [https://www.ibm.com/think/topics/bagging](https://www.ibm.com/think/topics/bagging)
10. jackknife_resampling — Astropy v7.2.dev631+g96a81bc91, accessed October 7, 2025, [https://docs.astropy.org/en/latest/api/astropy.stats.jackknife_resampling.html](https://docs.astropy.org/en/latest/api/astropy.stats.jackknife_resampling.html)
11. Bootstrap Estimates of Confidence Intervals - UVA Library - The University of Virginia, accessed October 7, 2025, [https://library.virginia.edu/data/articles/bootstrap-estimates-of-confidence-intervals](https://library.virginia.edu/data/articles/bootstrap-estimates-of-confidence-intervals)
12. Bootstrapping (statistics) - Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Bootstrapping_(statistics)](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))
13. Jackknife Resampling - GeeksforGeeks, accessed October 7, 2025, [https://www.geeksforgeeks.org/data-science/jackknife-resampling/](https://www.geeksforgeeks.org/data-science/jackknife-resampling/)
14. Jackknife Resampling - Biased-Algorithms - Medium, accessed October 7, 2025, [https://medium.com/biased-algorithms/jackknife-resampling-f33cf3f5d2eb](https://medium.com/biased-algorithms/jackknife-resampling-f33cf3f5d2eb)
15. Jackknife Resampling vs Bootstrap | by Amit Yadav | Biased ..., accessed October 7, 2025, [https://medium.com/biased-algorithms/jackknife-resampling-vs-bootstrap-5b2014576f56](https://medium.com/biased-algorithms/jackknife-resampling-vs-bootstrap-5b2014576f56)
16. How Does Jackknife Compare To Bootstrap? - The Friendly Statistician - YouTube, accessed October 7, 2025, [https://www.youtube.com/watch?v=xElec0mnoKA](https://www.youtube.com/watch?v=xElec0mnoKA)
17. Bootstrap Sampling Explained | CFA Level 1 - AnalystPrep, accessed October 7, 2025, [https://analystprep.com/cfa-level-1-exam/quantitative-methods/bootstrap-resampling/](https://analystprep.com/cfa-level-1-exam/quantitative-methods/bootstrap-resampling/)
18. Understanding the Bootstrap - CenterStat, accessed October 7, 2025, [https://centerstat.org/understanding-the-bootstrap/](https://centerstat.org/understanding-the-bootstrap/)
19. A Beginner's Guide to the Bootstrap - D-Lab, accessed October 7, 2025, [https://dlab.berkeley.edu/news/beginner%E2%80%99s-guide-bootstrap](https://dlab.berkeley.edu/news/beginner%E2%80%99s-guide-bootstrap)
20. Bootstrapping in Statistics – JACK TRAINER - Lancaster University, accessed October 7, 2025, [https://www.lancaster.ac.uk/stor-i-student-sites/jack-trainer/bootstrapping-in-statistics/](https://www.lancaster.ac.uk/stor-i-student-sites/jack-trainer/bootstrapping-in-statistics/)
21. Assumptions regarding bootstrap estimates of uncertainty - Cross Validated, accessed October 7, 2025, [https://stats.stackexchange.com/questions/11210/assumptions-regarding-bootstrap-estimates-of-uncertainty](https://stats.stackexchange.com/questions/11210/assumptions-regarding-bootstrap-estimates-of-uncertainty)
22. variance - Pros and cons of bootstrapping - Cross Validated - Stats StackExchange, accessed October 7, 2025, [https://stats.stackexchange.com/questions/280725/pros-and-cons-of-bootstrapping](https://stats.stackexchange.com/questions/280725/pros-and-cons-of-bootstrapping)
23. The Bootstrap, accessed October 7, 2025, [https://content.csbs.utah.edu/~rogers/tch/datanal/labprj/bootstrap/index.html](https://content.csbs.utah.edu/~rogers/tch/datanal/labprj/bootstrap/index.html)
24. Bootstrapping Medians - University of Vermont, accessed October 7, 2025, [https://www.uvm.edu/~statdhtx/StatPages/Randomization%20Tests/BootstMedians/bootstrapping_medians.html](https://www.uvm.edu/~statdhtx/StatPages/Randomization%20Tests/BootstMedians/bootstrapping_medians.html)
25. Bootstrap confidence intervals - Chan`s Jupyter, accessed October 7, 2025, [https://goodboychan.github.io/python/datacamp/data_science/statistics/2020/05/27/02-Bootstrap-confidence-intervals.html](https://goodboychan.github.io/python/datacamp/data_science/statistics/2020/05/27/02-Bootstrap-confidence-intervals.html)
26. bootstrap — SciPy v1.16.2 Manual, accessed October 7, 2025, [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html)
27. Bootstrap Aggregating and Random Forest - UCR | Department of Economics - University of California, Riverside, accessed October 7, 2025, [https://economics.ucr.edu/repec/ucr/wpaper/201918.pdf](https://economics.ucr.edu/repec/ucr/wpaper/201918.pdf)
28. Bootstrap aggregating - Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Bootstrap_aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating)
29. Bootstrap Aggregation, Random Forests and Boosted Trees | QuantStart, accessed October 7, 2025, [https://www.quantstart.com/articles/bootstrap-aggregation-random-forests-and-boosted-trees/](https://www.quantstart.com/articles/bootstrap-aggregation-random-forests-and-boosted-trees/)
30. cross validation - Jackknife vs. LOOCV - Cross Validated, accessed October 7, 2025, [https://stats.stackexchange.com/questions/144064/jackknife-vs-loocv](https://stats.stackexchange.com/questions/144064/jackknife-vs-loocv)
31. Bias-Variance Tradeoff & Cross-Validation | Statistical Prediction Class Notes | Fiveable, accessed October 7, 2025, [https://fiveable.me/modern-statistical-prediction-and-machine-learning/unit-3](https://fiveable.me/modern-statistical-prediction-and-machine-learning/unit-3)
32. Cross Validation - The Examples Book, accessed October 7, 2025, [https://the-examples-book.com/tools/data-modeling-cross-validation-loocv-kfold](https://the-examples-book.com/tools/data-modeling-cross-validation-loocv-kfold)
33. The Essential Guide to K-Fold Cross-Validation in Machine Learning ..., accessed October 7, 2025, [https://medium.com/@bididudy/the-essential-guide-to-k-fold-cross-validation-in-machine-learning-2bcb58c50578](https://medium.com/@bididudy/the-essential-guide-to-k-fold-cross-validation-in-machine-learning-2bcb58c50578)
34. 11. Common pitfalls and recommended practices - Scikit-learn, accessed October 7, 2025, [https://scikit-learn.org/stable/common_pitfalls.html](https://scikit-learn.org/stable/common_pitfalls.html)
35. Avoiding Data Leakage in Cross-Validation | by Silva.f.francis | Medium, accessed October 7, 2025, [https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0](https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0)
36. Prevent Data Leakage in Cross-Validation - Medium, accessed October 7, 2025, [https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553](https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553)
37. Cross validation & Hyperparameters Tuning - Kaggle, accessed October 7, 2025, [https://www.kaggle.com/code/abdokamr/cross-validation-hyperparameters-tuning](https://www.kaggle.com/code/abdokamr/cross-validation-hyperparameters-tuning)
38. Cross Validation in Time Series Forecasting | by Sushmita Poudel - Medium, accessed October 7, 2025, [https://medium.com/@poudelsushmita878/cross-validation-in-time-series-forecasting-db2bc7601875](https://medium.com/@poudelsushmita878/cross-validation-in-time-series-forecasting-db2bc7601875)
39. Cross Validation in Time Series. Cross Validation: | by Soumya ..., accessed October 7, 2025, [https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4](https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4)
40. Non i.i.d. data — Scikit-learn course, accessed October 7, 2025, [https://inria.github.io/scikit-learn-mooc/python_scripts/cross_validation_time.html](https://inria.github.io/scikit-learn-mooc/python_scripts/cross_validation_time.html)
41. Data Splitting Strategies — Applied Machine Learning in Python - Andreas Mueller, accessed October 7, 2025, [https://amueller.github.io/aml/04-model-evaluation/1-data-splitting-strategies.html](https://amueller.github.io/aml/04-model-evaluation/1-data-splitting-strategies.html)
